# -*- coding: utf-8 -*-
"""NLP_main_ver.ipynb

Automatically generated by Colaboratory.

Original file is located at
    https://colab.research.google.com/drive/1NGNAWGQ63KlKNdQ1Jgg2QUtJBAterh8v
"""

#!pip install textstat
#!pip install fasttext

import pandas as pd
import matplotlib.pyplot as plt
import numpy as np
import pprint
import time
from tqdm import tqdm
import datetime
import tweepy
import re
import os

import warnings
warnings.filterwarnings(action='ignore')

from textstat import flesch_reading_ease
import collections
import nltk
from nltk.corpus import stopwords
import string
import random
import xgboost as xgb
from sklearn.feature_extraction.text import TfidfVectorizer, CountVectorizer
from sklearn.decomposition import TruncatedSVD
from sklearn import metrics, model_selection, naive_bayes
from nltk.sentiment.vader import SentimentIntensityAnalyzer
from nltk.tokenize import word_tokenize
from nltk.tag import pos_tag
from nltk import word_tokenize, pos_tag, ne_chunk, tree2conlltags
import fasttext
from sklearn.linear_model import LogisticRegression
from sklearn.linear_model import SGDClassifier
from sklearn.ensemble import RandomForestClassifier
from sklearn.neural_network import MLPClassifier
from sklearn.tree import DecisionTreeClassifier

from sklearn.model_selection import StratifiedShuffleSplit
from sklearn import preprocessing


class NLP:
  ticker_account1 = {'ETH':'ethereum', 'BTC':'Bitcoin', 'DOGE':'doge', 'AXS':'AxieInfinity', 'ADA':'Cardano', 'XRP':'Ripple', 'DOT':'Polkadot', 'SAND':'TheSandboxGame', 'LINK':'chainlink',
                    'THETA':'Theta_Network', 'ETC':'eth_classic', 'LTC':'litecoin', 'SNX':'synthetix_io', 'CHZ':'Chiliz', 'VET':'vechainofficial', 'FIL':'protocollabs', 'COMP':'compoundfinance',
                    'ATOM':'cosmos', 'BCH':'bitcoincashorg', 'TRX':'Tronfoundation', 'MANA':'decentraland', 'TFUEL':'Theta_Network', 'ENJ':'enjin', 'ZRX':'0xProject', 'ICX':'helloiconworld',
                    'STX':'Stacks', 'OXT':'OrchidProtocol', 'XLM':'StellarOrg', 'GRT':'graphprotocol', 'NEO':'Neo_Blockchain', 'HBAR':'hedera', 'HIVE':'hiveblock',
                    'ONT':'OntologyNetwork', 'OMG':'omgnetworkhq', 'PUNDIX':'PundiXLabs', 'INJ':'InjectiveLabs', 'KAVA':'kava_labs', 'OGN':'OriginProtocol', 'XTZ':'tezos ', 'LINA':'LinearFinance',}
  ticker_account2= {'CRV':'CurveFinance', 'STRAX':'stratisplatform', 'BTT':'BitTorrent', 'DENT':'dentcoin', 'OXT':'OrchidProtocol', 'PSG':'socios', 'WAVES':'wavesprotocol', 'STMX':'stormxio',
                    'IOST':'IOST_Official', 'NEAR':'NEARProtocol', 'ALGO':'AlgoFoundation', 'CELO':'CeloOrg', 'LSK':'LiskHQ', 'QTUM':'qtum', 'ZIL':'zilliqa', 'MKR':'MakerDAO', 'RSR':'reserveprotocol',
                    'RLC':'iEx_ec', 'TUSD':'TrustToken', 'NKN':'NKN_ORG', 'RVN':'Ravencoin', 'XEM':'NEMofficial', 'SC':'Sia__Foundation', 'BAT':'AttentionToken', 'PAX':'PaxosGlobal',
                    'CTSI':'cartesiproject', 'STORJ':'storj', 'JUV':'socios', 'DGB':'DigiByteCoin', 'ANKR':'ankr', 'MFT':'HifiFinance', 'CVC':'civickey', 'LRC':'loopringorg', 'STPT':'STP_Networks',
                    'JST':'DeFi_JUST', 'ARDR':'ArdorPlatform', 'IOTX':'iotex_io', 'BNT':'Bancor', 'EOS':'EOSIO', 'LUNA':'terra_money', 'UNI':'Uniswap', 'VET':'vechainofficial', }

  tickers1 = [i+'/USDT' for i in ticker_account1.keys()]
  tickers2 = [i+'/USDT' for i in ticker_account2.keys()]
  tickers = list(set(tickers1+tickers2))

  """- 경로 수정해주시면 됩니다!!"""

  dataset = pd.read_parquet('') # total_df2.parquet 경로 추가 

  nltk.download('stopwords')
  nltk.download('averaged_perceptron_tagger')
  nltk.download('punkt')
  nltk.download('vader_lexicon')
  nltk.download('maxent_ne_chunker')
  nltk.download('words')

  eng_stopwords = set(stopwords.words("english"))
  symbols_knowns = string.ascii_letters + string.digits + string.punctuation
  good_words = ['love', 'users', 'ears', 'open', 'space', 'last', 'smart', 'trading', 'world', 'soon', 'network', 'great', 'every',]

  def clean_text(text):
    # 텍스트 기본 전처리
    
    # 트윗 말단 주소 제거
    text = re.sub("http.*", '', text)
    text = re.sub("https.*", '', text)
    # RT 대상 account 제거
    text = re.sub('RT @.*:', 'RT', text)
    # 불용어 제거(!, $ 제외)
    text = re.sub('[^a-zA-Z$!0-9 ]', '', text)

    # 소문자 변환
    text = text.lower()

    return text

  def get_words(text):
      words = nltk.tokenize.word_tokenize(text)
      return [word for word in words]

  def sentiment_nltk(text):
      res = SentimentIntensityAnalyzer().polarity_scores(text)
      return res['compound']

  def make_features(dataset,self):
    dataset['ntext'] = dataset['text'].apply(lambda x : self.clean_text(x))
    dataset['num_words'] = dataset['ntext'].apply(lambda x: len(self.get_words(x)))
    dataset['num_unique_words'] = dataset['ntext'].apply(lambda x: len(set(str(x).split())))
    dataset['num_chars'] = dataset['ntext'].apply(lambda x: len(str(x)))
    dataset['num_exmark'] = dataset['ntext'].apply(lambda x: len([e for e in self.get_words(x) if e == '!']))/(dataset['num_words']+1e-8)
    dataset['num_dollar'] = dataset['ntext'].apply(lambda x: len([e for e in self.get_words(x) if e == '$']))/(dataset['num_words']+1e-8)
    dataset['is_rt'] = dataset['ntext'].apply(lambda x: 1 if x.startswith('rt') else 0)
    dataset['num_good_words'] = dataset['ntext'].apply(lambda x: len([g for g in self.get_words(x) if g in self.good_words]))/(dataset['num_words']+1e-8)
    dataset['sentiment'] = dataset['ntext'].apply(self.sentiment_nltk)
    dataset['ease'] = dataset['ntext'].apply(flesch_reading_ease)

    return dataset

  def make_coin_name(dataset,self):
    for i in range(len(dataset)):
      coin_words = []
      coin_words += [i for i in self.get_words(dataset.loc[i, 'account'].lower())]
      coin_words.append(dataset.loc[i, 'ticker'].lower())

      num = 0

      for j in coin_words:
        if j in dataset.loc[i, 'ntext']:
          num += 1

      dataset.loc[i, 'coin_name'] = num

    dataset['coin_name'] = dataset['coin_name'].apply(lambda x: 1 if x != 0 else 0)

    return dataset

  dataset = make_features(dataset)

  dataset = make_coin_name(dataset)

  all_data = dataset.drop(['time', 'account', 'ticker', 'text'], axis = 1)

  

  split = StratifiedShuffleSplit(n_splits=1, test_size=0.3, random_state=2)

  for train_index, test_index in split.split(all_data, all_data["target"]):
      train = all_data.loc[train_index]
      test = all_data.loc[test_index]

  train.reset_index(drop = True, inplace = True)
  test.reset_index(drop = True, inplace = True)

  X_train, y_train = train.drop(['target', 'ntext'], axis = 1), train['target']
  X_test, y_test = test.drop(['target', 'ntext'], axis=1), test['target']



  def runLR(train_X, train_y, test_X, test_y, test_X2):
      model=LogisticRegression(penalty = 'l2', class_weight = 'balanced', multi_class = 'ovr')
      model.fit(train_X,train_y)
      pred_test_y=model.predict_proba(test_X)
      pred_test_y2=model.predict_proba(test_X2)
      return pred_test_y, pred_test_y2, model

  def runSGD(train_X,train_y,test_X,test_y,test_X2):
      model=SGDClassifier(penalty = 'l2', loss='log')
      model.fit(train_X,train_y)
      pred_test_y=model.predict_proba(test_X)
      pred_test_y2=model.predict_proba(test_X2)
      return pred_test_y, pred_test_y2, model

  def runMLP(train_X,train_y,test_X,test_y,test_X2):
      model=MLPClassifier(activation='logistic')
      model.fit(train_X,train_y)
      pred_test_y=model.predict_proba(test_X)
      pred_test_y2=model.predict_proba(test_X2)
      return pred_test_y, pred_test_y2, model

  def runMNB(train_X,train_y,test_X,test_y,test_X2):
      model=naive_bayes.MultinomialNB()
      model.fit(train_X,train_y)
      pred_test_y=model.predict_proba(test_X)
      pred_test_y2=model.predict_proba(test_X2)
      return pred_test_y, pred_test_y2, model

  def runRF(train_X,train_y,test_X,test_y,test_X2):
      model=RandomForestClassifier(class_weight='balanced')
      model.fit(train_X,train_y)
      pred_test_y=model.predict_proba(test_X)
      pred_test_y2=model.predict_proba(test_X2)
      return pred_test_y, pred_test_y2, model

  def runDT(train_X,train_y,test_X,test_y,test_X2):
      model=DecisionTreeClassifier(class_weight='balanced')
      model.fit(train_X,train_y)
      pred_test_y=model.predict_proba(test_X)
      pred_test_y2=model.predict_proba(test_X2)
      return pred_test_y, pred_test_y2, model

  def sent2vec(s,self):
      words = nltk.tokenize.word_tokenize(s)
      #words = [k.stem(w) for w in words]
      #words = [w for w in words if not w in string.digits]
      #words = [w for w in words if w.isalpha()]
      M = []
      for w in words:
          try:
              M.append(self.model_ft[w])
          except:
              continue
      M = np.array(M)
      v = M.sum(axis=0)
      if type(v) != np.ndarray:
          return np.zeros(300)
      return v

  tfidf_vec = TfidfVectorizer(tokenizer=word_tokenize, stop_words=stopwords.words('english'), ngram_range=(1, 3), min_df=100)
  train_tfidf = tfidf_vec.fit_transform(train['ntext'].values.tolist())
  test_tfidf = tfidf_vec.transform(test['ntext'].values.tolist())

  cvec_vec=CountVectorizer(tokenizer=word_tokenize, stop_words=stopwords.words('english'), ngram_range=(1, 3), min_df=100)
  cvec_vec.fit(train['ntext'].values.tolist())
  train_cvec = cvec_vec.transform(train['ntext'].values.tolist())
  test_cvec = cvec_vec.transform(test['ntext'].values.tolist())

  cvec_char_vec = CountVectorizer(ngram_range=(1,7), analyzer='char')
  cvec_char_vec.fit(train['ntext'].values.tolist())
  train_cvec_char = cvec_char_vec.transform(train['ntext'].values.tolist())
  test_cvec_char = cvec_char_vec.transform(test['ntext'].values.tolist())

  cv_scores=[]
  pred_train=np.zeros([train.shape[0], 2])
  pred_full_test = 0
  cv = model_selection.StratifiedKFold(n_splits=5, shuffle=True, random_state=2021)

  for dev_index, val_index in cv.split(X_train, y_train):
      dev_X, val_X = train_tfidf[dev_index], train_tfidf[val_index]
      dev_y, val_y = y_train[dev_index], y_train[val_index]
      pred_val_y, pred_test_y, model_LR_tfidf = runLR(dev_X, dev_y, val_X, val_y, test_tfidf)
      pred_full_test = pred_full_test + pred_test_y
      pred_train[val_index,:] = pred_val_y
      cv_scores.append(metrics.log_loss(val_y, pred_val_y))

  #print("Mean cv score - tfidf : ", np.mean(cv_scores))
  pred_full_test = pred_full_test / 5.

  train["tfidf_LR_0"] = pred_train[:,0]
  train["tfidf_LR_1"] = pred_train[:,1]

  test["tfidf_LR_0"] = pred_full_test[:,0]
  test["tfidf_LR_1"] = pred_full_test[:,1]

  cv_scores=[]
  pred_train=np.zeros([train.shape[0], 2])
  pred_full_test = 0

  cv = model_selection.StratifiedKFold(n_splits=5, shuffle=True, random_state=2021)

  for dev_index, val_index in cv.split(X_train, y_train):
      dev_X, val_X = train_cvec[dev_index], train_cvec[val_index]
      dev_y, val_y = y_train[dev_index], y_train[val_index]
      pred_val_y, pred_test_y, model_LR_cvec = runLR(dev_X, dev_y, val_X, val_y,test_cvec)
      pred_full_test = pred_full_test + pred_test_y
      pred_train[val_index,:] = pred_val_y
      cv_scores.append(metrics.log_loss(val_y, pred_val_y))

  #print("Mean cv score - count: ", np.mean(cv_scores))
  pred_full_test = pred_full_test / 5.

  train["cvec_LR_0"] = pred_train[:,0]
  train["cvec_LR_1"] = pred_train[:,1]

  test["cvec_LR_0"] = pred_full_test[:,0]
  test["cvec_LR_1"] = pred_full_test[:,1]

  cv_scores=[]
  pred_train=np.zeros([train.shape[0], 2])
  pred_full_test = 0
  cv = model_selection.StratifiedKFold(n_splits=5, shuffle=True, random_state=2021)

  for dev_index, val_index in cv.split(X_train, y_train):
      dev_X, val_X = train_tfidf[dev_index], train_tfidf[val_index]
      dev_y, val_y = y_train[dev_index], y_train[val_index]
      pred_val_y, pred_test_y, model_SGD_tfidf = runSGD(dev_X, dev_y, val_X, val_y, test_tfidf)
      pred_full_test = pred_full_test + pred_test_y
      pred_train[val_index,:] = pred_val_y
      cv_scores.append(metrics.log_loss(val_y, pred_val_y))

  #print("Mean cv score - tfidf : ", np.mean(cv_scores))
  pred_full_test = pred_full_test / 5.

  train["tfidf_SGD_0"] = pred_train[:,0]
  train["tfidf_SGD_1"] = pred_train[:,1]

  test["tfidf_SGD_0"] = pred_full_test[:,0]
  test["tfidf_SGD_1"] = pred_full_test[:,1]

  cv_scores=[]
  pred_train=np.zeros([train.shape[0], 2])
  pred_full_test = 0

  cv = model_selection.StratifiedKFold(n_splits=5, shuffle=True, random_state=2021)

  for dev_index, val_index in cv.split(X_train, y_train):
      dev_X, val_X = train_cvec[dev_index], train_cvec[val_index]
      dev_y, val_y = y_train[dev_index], y_train[val_index]
      pred_val_y, pred_test_y, model_SGD_cvec = runSGD(dev_X, dev_y, val_X, val_y,test_cvec)
      pred_full_test = pred_full_test + pred_test_y
      pred_train[val_index,:] = pred_val_y
      cv_scores.append(metrics.log_loss(val_y, pred_val_y))

  #print("Mean cv score - count: ", np.mean(cv_scores))
  pred_full_test = pred_full_test / 5.

  train["cvec_SGD_0"] = pred_train[:,0]
  train["cvec_SGD_1"] = pred_train[:,1]

  test["cvec_SGD_0"] = pred_full_test[:,0]
  test["cvec_SGD_1"] = pred_full_test[:,1]

  cv_scores=[]
  pred_train=np.zeros([train.shape[0], 2])
  pred_full_test = 0
  cv = model_selection.StratifiedKFold(n_splits=5, shuffle=True, random_state=2021)

  for dev_index, val_index in cv.split(X_train, y_train):
      dev_X, val_X = train_tfidf[dev_index], train_tfidf[val_index]
      dev_y, val_y = y_train[dev_index], y_train[val_index]
      pred_val_y, pred_test_y, model_MLP_tfidf = runMLP(dev_X, dev_y, val_X, val_y, test_tfidf)
      pred_full_test = pred_full_test + pred_test_y
      pred_train[val_index,:] = pred_val_y
      cv_scores.append(metrics.log_loss(val_y, pred_val_y))

  #print("Mean cv score - tfidf : ", np.mean(cv_scores))
  pred_full_test = pred_full_test / 5.

  train["tfidf_MLP_0"] = pred_train[:,0]
  train["tfidf_MLP_1"] = pred_train[:,1]

  test["tfidf_MLP_0"] = pred_full_test[:,0]
  test["tfidf_MLP_1"] = pred_full_test[:,1]

  cv_scores=[]
  pred_train=np.zeros([train.shape[0], 2])
  pred_full_test = 0

  cv = model_selection.StratifiedKFold(n_splits=5, shuffle=True, random_state=2021)

  for dev_index, val_index in cv.split(X_train, y_train):
      dev_X, val_X = train_cvec[dev_index], train_cvec[val_index]
      dev_y, val_y = y_train[dev_index], y_train[val_index]
      pred_val_y, pred_test_y, model_MLP_cvec = runMLP(dev_X, dev_y, val_X, val_y,test_cvec)
      pred_full_test = pred_full_test + pred_test_y
      pred_train[val_index,:] = pred_val_y
      cv_scores.append(metrics.log_loss(val_y, pred_val_y))

  #print("Mean cv score - count: ", np.mean(cv_scores))
  pred_full_test = pred_full_test / 5.

  train["cvec_MLP_0"] = pred_train[:,0]
  train["cvec_MLP_1"] = pred_train[:,1]

  test["cvec_MLP_0"] = pred_full_test[:,0]
  test["cvec_MLP_1"] = pred_full_test[:,1]

  cv_scores=[]
  pred_train=np.zeros([train.shape[0], 2])
  pred_full_test = 0
  cv = model_selection.StratifiedKFold(n_splits=5, shuffle=True, random_state=2021)

  for dev_index, val_index in cv.split(X_train, y_train):
      dev_X, val_X = train_tfidf[dev_index], train_tfidf[val_index]
      dev_y, val_y = y_train[dev_index], y_train[val_index]
      pred_val_y, pred_test_y, model_MNB_tfidf = runMNB(dev_X, dev_y, val_X, val_y, test_tfidf)
      pred_full_test = pred_full_test + pred_test_y
      pred_train[val_index,:] = pred_val_y
      cv_scores.append(metrics.log_loss(val_y, pred_val_y))

  #print("Mean cv score - tfidf : ", np.mean(cv_scores))
  pred_full_test = pred_full_test / 5.

  train["tfidf_MNB_0"] = pred_train[:,0]
  train["tfidf_MNB_1"] = pred_train[:,1]

  test["tfidf_MNB_0"] = pred_full_test[:,0]
  test["tfidf_MNB_1"] = pred_full_test[:,1]

  cv_scores=[]
  pred_train=np.zeros([train.shape[0], 2])
  pred_full_test = 0

  cv = model_selection.StratifiedKFold(n_splits=5, shuffle=True, random_state=2021)

  for dev_index, val_index in cv.split(X_train, y_train):
      dev_X, val_X = train_cvec[dev_index], train_cvec[val_index]
      dev_y, val_y = y_train[dev_index], y_train[val_index]
      pred_val_y, pred_test_y, model_MNB_cvec = runMNB(dev_X, dev_y, val_X, val_y,test_cvec)
      pred_full_test = pred_full_test + pred_test_y
      pred_train[val_index,:] = pred_val_y
      cv_scores.append(metrics.log_loss(val_y, pred_val_y))

  #print("Mean cv score - count: ", np.mean(cv_scores))
  pred_full_test = pred_full_test / 5.

  train["cvec_MNB_0"] = pred_train[:,0]
  train["cvec_MNB_1"] = pred_train[:,1]

  test["cvec_MNB_0"] = pred_full_test[:,0]
  test["cvec_MNB_1"] = pred_full_test[:,1]

  cv_scores=[]
  pred_train=np.zeros([train.shape[0], 2])
  pred_full_test = 0
  cv = model_selection.StratifiedKFold(n_splits=5, shuffle=True, random_state=2021)

  for dev_index, val_index in cv.split(X_train, y_train):
      dev_X, val_X = train_tfidf[dev_index], train_tfidf[val_index]
      dev_y, val_y = y_train[dev_index], y_train[val_index]
      pred_val_y, pred_test_y, model_RF_tfidf = runRF(dev_X, dev_y, val_X, val_y, test_tfidf)
      pred_full_test = pred_full_test + pred_test_y
      pred_train[val_index,:] = pred_val_y
      cv_scores.append(metrics.log_loss(val_y, pred_val_y))

  #print("Mean cv score - tfidf : ", np.mean(cv_scores))
  pred_full_test = pred_full_test / 5.

  train["tfidf_RF_0"] = pred_train[:,0]
  train["tfidf_RF_1"] = pred_train[:,1]

  test["tfidf_RF_0"] = pred_full_test[:,0]
  test["tfidf_RF_1"] = pred_full_test[:,1]

  cv_scores=[]
  pred_train=np.zeros([train.shape[0], 2])
  pred_full_test = 0

  cv = model_selection.StratifiedKFold(n_splits=5, shuffle=True, random_state=2021)

  for dev_index, val_index in cv.split(X_train, y_train):
      dev_X, val_X = train_cvec[dev_index], train_cvec[val_index]
      dev_y, val_y = y_train[dev_index], y_train[val_index]
      pred_val_y, pred_test_y, model_RF_cvec = runRF(dev_X, dev_y, val_X, val_y,test_cvec)
      pred_full_test = pred_full_test + pred_test_y
      pred_train[val_index,:] = pred_val_y
      cv_scores.append(metrics.log_loss(val_y, pred_val_y))

  #print("Mean cv score - count: ", np.mean(cv_scores))
  pred_full_test = pred_full_test / 5.

  train["cvec_RF_0"] = pred_train[:,0]
  train["cvec_RF_1"] = pred_train[:,1]

  test["cvec_RF_0"] = pred_full_test[:,0]
  test["cvec_RF_1"] = pred_full_test[:,1]

  cv_scores=[]
  pred_train=np.zeros([train.shape[0], 2])
  pred_full_test = 0
  cv = model_selection.StratifiedKFold(n_splits=5, shuffle=True, random_state=2021)

  for dev_index, val_index in cv.split(X_train, y_train):
      dev_X, val_X = train_tfidf[dev_index], train_tfidf[val_index]
      dev_y, val_y = y_train[dev_index], y_train[val_index]
      pred_val_y, pred_test_y, model_DT_tfidf = runDT(dev_X, dev_y, val_X, val_y, test_tfidf)
      pred_full_test = pred_full_test + pred_test_y
      pred_train[val_index,:] = pred_val_y
      cv_scores.append(metrics.log_loss(val_y, pred_val_y))

  #print("Mean cv score - tfidf : ", np.mean(cv_scores))
  pred_full_test = pred_full_test / 5.

  train["tfidf_DT_0"] = pred_train[:,0]
  train["tfidf_DT_1"] = pred_train[:,1]

  test["tfidf_DT_0"] = pred_full_test[:,0]
  test["tfidf_DT_1"] = pred_full_test[:,1]

  cv_scores=[]
  pred_train=np.zeros([train.shape[0], 2])
  pred_full_test = 0

  cv = model_selection.StratifiedKFold(n_splits=5, shuffle=True, random_state=2021)

  for dev_index, val_index in cv.split(X_train, y_train):
      dev_X, val_X = train_cvec[dev_index], train_cvec[val_index]
      dev_y, val_y = y_train[dev_index], y_train[val_index]
      pred_val_y, pred_test_y, model_DT_evec = runDT(dev_X, dev_y, val_X, val_y,test_cvec)
      pred_full_test = pred_full_test + pred_test_y
      pred_train[val_index,:] = pred_val_y
      cv_scores.append(metrics.log_loss(val_y, pred_val_y))

  #print("Mean cv score - count: ", np.mean(cv_scores))
  pred_full_test = pred_full_test / 5.

  train["cvec_DT_0"] = pred_train[:,0]
  train["cvec_DT_1"] = pred_train[:,1]

  test["cvec_DT_0"] = pred_full_test[:,0]
  test["cvec_DT_1"] = pred_full_test[:,1]

  n_comp = 3
  svd_obj = TruncatedSVD(n_components=n_comp, algorithm='arpack')
  svd_obj.fit(train_tfidf)

  train_svd = svd_obj.transform(train_tfidf)
  test_svd = svd_obj.transform(test_tfidf)

  scl = preprocessing.StandardScaler()
  scl.fit(train_svd)
  train_svd_scl = pd.DataFrame(scl.transform(train_svd))
  test_svd_scl = pd.DataFrame(scl.transform(test_svd))

  train_svd_scl.columns = ['svd_word_'+str(i) for i in range(n_comp)]
  test_svd_scl.columns = ['svd_word_'+str(i) for i in range(n_comp)]
  train = pd.concat([train, train_svd_scl], axis=1)
  test = pd.concat([test, test_svd_scl], axis=1)

  train.drop(['close', 'volume', 'direction', 'plus', 'ma7', 'ma25', 'vma7', 'vma25', 'macd7_25', 'sep7', 'v_sep7', 'mfi7',
              'tfidf_LR_0', 'tfidf_LR_1', 'cvec_LR_0', 'cvec_LR_1', 'tfidf_SGD_0', 'cvec_SGD_0', 'tfidf_MLP_0', 'cvec_MLP_0', 'tfidf_MNB_0', 'cvec_MNB_0',
              'tfidf_RF_0', 'tfidf_RF_1', 'cvec_RF_0', 'cvec_RF_1', 'tfidf_DT_0', 'tfidf_DT_1', 'cvec_DT_0', 'cvec_DT_1'], axis = 1, inplace = True)

  test.drop(['close', 'volume', 'direction', 'plus', 'ma7', 'ma25', 'vma7', 'vma25', 'macd7_25', 'sep7', 'v_sep7', 'mfi7',
              'tfidf_LR_0', 'tfidf_LR_1', 'cvec_LR_0', 'cvec_LR_1', 'tfidf_SGD_0', 'cvec_SGD_0', 'tfidf_MLP_0', 'cvec_MLP_0', 'tfidf_MNB_0', 'cvec_MNB_0',
              'tfidf_RF_0', 'tfidf_RF_1', 'cvec_RF_0', 'cvec_RF_1', 'tfidf_DT_0', 'tfidf_DT_1', 'cvec_DT_0', 'cvec_DT_1'], axis = 1, inplace = True)

  cols_to_drop = ['target', 'ntext']
  train_X = train.drop(cols_to_drop, axis=1)
  train_y=train['target']
  test_X = test.drop(cols_to_drop, axis=1)
  xgb_preds=[]

  kf = model_selection.KFold(n_splits=5, shuffle=True, random_state=2020)

  for dev_index, val_index in kf.split(train_X):
      dev_X, val_X = train_X.loc[dev_index], train_X.loc[val_index]
      dev_y, val_y = train_y[dev_index], train_y[val_index]
      dtrain = xgb.DMatrix(dev_X,label=dev_y)
      dvalid = xgb.DMatrix(val_X, label=val_y)
      watchlist = [(dtrain, 'train'), (dvalid, 'valid')]

      param = {}
      param['objective'] = 'binary:logistic'
      param['eta'] = 0.1
      param['max_depth'] = 3
      param['silent'] = 1
      param['eval_metric'] = "aucpr"
      param['min_child_weight'] = 1
      param['subsample'] = 0.7
      param['colsample_bytree'] = 0.7
      param['seed'] = 21
      param['scale_pos_weigth'] = 100

      model = xgb.train(param, dtrain, 2000, watchlist, early_stopping_rounds=50, verbose_eval=50)

      xgtest2 = xgb.DMatrix(test_X)
      xgb_pred = model.predict(xgtest2, ntree_limit = model.best_ntree_limit)
      xgb_preds.append(list(xgb_pred))

  import json
  from tweepy.streaming import StreamListener
  from tweepy import OAuthHandler
  from tweepy import Stream
  import time

  TWITTER_CONSUMER_KEY='9ule1jHK8i86xJcEadAnjINWC'
  TWITTER_CONSUMER_SECRET="aeT580KZjXAL1p7alZCu59PsQXZ6EHvA7OsNcM5wSTuBeIe4yP"
  TWITTER_ACCESS_TOKEN="1408441498417307654-QI0xIJiv1iAS1rrIHeXXnZGiHBuRgx"
  TWITTER_ACCESS_TOKEN_SECRET="JamfuYyKGAWfGOLskHn1FLIBIImkITagHFisgwEsM0Ioo"

  auth = tweepy.OAuthHandler(TWITTER_CONSUMER_KEY, TWITTER_CONSUMER_SECRET)
  auth.set_access_token(TWITTER_ACCESS_TOKEN, TWITTER_ACCESS_TOKEN_SECRET)
  api = tweepy.API(auth,wait_on_rate_limit=True)

  id_list = collections.defaultdict(str)

  for k, v in ticker_account1.items():
    try:
      user = api.get_user(v)
      id_list[k] = user.id_str
      #id_list.append(user.id_str)
    except:
      pass
      
  for k, v in ticker_account2.items():
    user = api.get_user(v)
    id_list[k] = user.id_str
    #id_list.append(user.id_str)

  ticker_account1_r = dict(map(reversed, ticker_account1.items()))
  ticker_account2_r = dict(map(reversed, ticker_account2.items()))



class MyStreamListener(tweepy.StreamListener,NLP):
  def __init__(self, time_limit=60):
      self.start_time = time.time()
      self.limit = time_limit
      tm = time.localtime(self.start_time)
      string = time.strftime('%Y-%m-%d %I:%M:%S %p', tm)
      print('Start Time :', string)
      super(MyStreamListener, self).__init__()

  def on_status(self, status):

    if (time.time() - self.start_time) < self.limit:

      when = re.sub('[-: ]', "", str(status.created_at.strftime("%Y-%m-%d-%H:%M:%S"))[:-3])

      try:
        text = status.extended_tweet['full_text']

      except:
        text = status.text

      account = status.user.screen_name

      if account in list(NLP.ticker_account1.values()):
        ticker = NLP.ticker_account1_r[account]
        line = {'time':time, 'account':account, 'ticker':ticker, 'text':text}
        #line = {'time':when, 'account':account, 'text':text}
        NLP.csv.append(line)

      elif account in list(NLP.ticker_account2.values()):
        ticker = NLP.ticker_account2_r[account]
        line = {'time':time, 'account':account, 'ticker':ticker, 'text':text}
        #line = {'time':when, 'account':account, 'text':text}
        NLP.csv.append(line)

      return True
      
    else:
      return False

  def on_error(self, status_code):
       print(status_code)


class NLP2(NLP):
  def real_time():
    myStream = NLP.Stream(NLP.auth, MyStreamListener())
    myStream.filter(follow=list(NLP.id_list.values()), languages = ['en'])

  while True:
    csv = []
    real_time()
    real_df = pd.DataFrame(csv)

    if len(real_df) > 0:
      real_df = NLP.make_features(real_df)
      real_df_ = real_df.copy()

      test_tfidf = NLP.tfidf_vec.transform(real_df['ntext'].values.tolist())
      test_cvec = NLP.cvec_vec.transform(real_df['ntext'].values.tolist())
      test_cvec_char = NLP.cvec_char_vec.transform(real_df['ntext'].values.tolist())

      real_df['tfidf_SGD_1'] = NLP.model_SGD_tfidf.predict_proba(test_tfidf)[:, 1]
      real_df['cvec_SGD_1'] = NLP.model_SGD_cvec.predict_proba(test_cvec)[:, 1]
      real_df['tfidf_MLP_1'] = NLP.model_MLP_tfidf.predict_proba(test_tfidf)[:, 1]
      real_df['cvec_MLP_1'] = NLP.model_MLP_cvec.predict_proba(test_cvec)[:, 1]
      real_df['tfidf_MNB_1'] = NLP.model_MNB_tfidf.predict_proba(test_tfidf)[:, 1]
      real_df['cvec_MNB_1'] = NLP.model_MNB_cvec.predict_proba(test_cvec)[:, 1]
      test_svd = NLP.svd_obj.transform(test_tfidf)
      test_svd_scl = pd.DataFrame(NLP.scl.transform(test_svd))
      test_svd_scl.columns = ['svd_word_'+str(i) for i in range(NLP.n_comp)]
      real_df = pd.concat([real_df, test_svd_scl], axis=1)

      real_df.drop(['time', 'account', 'ticker', 'text', 'ntext'], axis = 1, inplace = True)
      xgb_preds=[]
      real_xgbtest = xgb.DMatrix(real_df)
      xgb_pred = NLP.model.predict(real_xgbtest, ntree_limit = NLP.model.best_ntree_limit)
      xgb_preds.append(list(xgb_pred))

      real_df_['result'] = [1 if i > 0.5 else 0 for i in xgb_preds[0]]
      real_df_1 = real_df_[real_df_['result'] ==1]

      if len(real_df_1) > 0:
        print(real_df_1[['time', 'ticker', 'result']])

      else:
        print('--- No Signal ---')

    else:
      print('--- No Tweets ---')

